{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up: import libraries and read data"
      ],
      "metadata": {
        "id": "veANy5oTGJar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import cross_val_predict, cross_validate\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve,precision_score, accuracy_score, recall_score, f1_score\n",
        "from sklearn.metrics import\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjiFnI5OFz9V",
        "outputId": "99bc4f77-2e49-4673-c2bc-db64a1d05315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = pd.read_csv('../Datasets/sentiment.csv')\n",
        "sentiment = sentiment.dropna()\n",
        "X_train = sentiment.drop(columns = ['label', 'parent_comment'])\n",
        "y_train = sentiment['label']"
      ],
      "metadata": {
        "id": "vcsd8zxZImba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "Xp8O2ytXJcMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits = 5, shuffle = True, random_state = 123)\n",
        "acc = []\n",
        "prec = []\n",
        "rec = []\n",
        "f1 = []\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for train_i, val_i in kf.split(X_train):\n",
        "  X_train_fold, X_val_fold = X_train.iloc[train_i], X_train.iloc[val_i] # numpy array\n",
        "  y_train_fold, y_val_fold = y_train.iloc[train_i], y_train.iloc[val_i] # pd df\n",
        "\n",
        "  # Apply Tf-idf vectors on comments\n",
        "  tfidf = TfidfVectorizer(min_df = 15)\n",
        "  train_tfidf = tfidf.fit_transform(X_train_fold[\"comment\"])\n",
        "  val_tfidf = tfidf.transform(X_val_fold[\"comment\"])\n",
        "\n",
        "  X_train_tfidf = hstack([csr_matrix(X_train_fold.drop(['comment'], axis =1).values), train_tfidf])\n",
        "  X_val_tfidf = hstack([csr_matrix(X_val_fold.drop(['comment'], axis = 1).values), val_tfidf])\n",
        "\n",
        "  # Scale features before applying PCA\n",
        "  scaler = StandardScaler()\n",
        "  X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf.toarray())\n",
        "  X_val_tfidf_scaled = scaler.transform(X_val_tfidf.toarray())\n",
        "\n",
        "  # Apply PCA\n",
        "  pca = PCA(n_components = 0.95)\n",
        "  X_train_pca = pca.fit_transform(X_train_tfidf_scaled)\n",
        "  X_val_pca = pca.transform(X_val_tfidf_scaled)\n",
        "\n",
        "  log_reg = LogisticRegression(random_state = 123, max_iter = 1000)\n",
        "  log_reg.fit(X_train_pca, y_train_fold)\n",
        "  preds = log_reg.predict(X_val_pca)\n",
        "  y_pred.extend(preds)\n",
        "  y_true.extend(y_val_fold)\n",
        "  acc.append(accuracy_score(y_val_fold, preds))\n",
        "  prec.append(precision_score(y_val_fold, preds))\n",
        "  rec.append(recall_score(y_val_fold, preds))\n",
        "  f1.append(f1_score(y_val_fold, preds))\n",
        "\n"
      ],
      "metadata": {
        "id": "aiKeKoST6mMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean accuracy: {np.mean(acc)}')\n",
        "print(f'Mean precision: {np.mean(prec)}')\n",
        "print(f'Mean recall: {np.mean(rec)}')\n",
        "print(f'Mean f1: {np.mean(f1)}')\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XdqhXL4JQu0",
        "outputId": "692c8f9b-5ad4-4e2d-8b3d-346a04f6783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.6664622366691855\n",
            "Mean precision: 0.6990595535005332\n",
            "Mean recall: 0.8108624238913645\n",
            "Mean f1: 0.7508128981464242\n",
            "Confusion matrix:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13367, 17636],\n",
              "       [ 9555, 40965]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "stUdzKr6Jb-s",
        "WwkjfmW6gFp9",
        "SBSfI_SQqVPh",
        "3qfCxhYZLvCw",
        "iBU1gtfKV26E",
        "4cJYaVKs_CMQ",
        "ayHlE6_e5knD",
        "A_3xeqKQ87XE",
        "YFhjnaqFgvPq",
        "ZazYQ8_MqsfZ",
        "sOF8LounrE_a",
        "fTsydv6hgT-H",
        "nu5qQt3yY44E",
        "gRvsRYtRzwDE",
        "Li9NeHxZYzts"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
